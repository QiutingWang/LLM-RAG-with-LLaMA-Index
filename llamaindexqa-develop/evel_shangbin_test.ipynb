{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sunshangbin.p/anaconda3/envs/llama/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\u001b[32m2023-11-27 17:45:03.515\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconfig\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m33\u001b[0m - \u001b[34m\u001b[1mConfig: {'API_BASE': 'https://gf.nekoapi.com/v1', 'MODEL_NAME': ['gpt-4 ', 'Baichuan-13B-Chat'], 'EMBEDDING_NAME': ['text-embedding-ada-002'], 'API_KEYS': 'sk-aJzbu0F3j7bstWlR3e4cA9Db59Ac4f669a9f471aFa66C458'}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.width\", None)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "import os \n",
    "import sys\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.getcwd())))\n",
    "\n",
    "import pandas as pd\n",
    "import nest_asyncio\n",
    "import os\n",
    "nest_asyncio.apply()\n",
    "from typing import Optional\n",
    "\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index import (\n",
    "    ServiceContext,\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "\n",
    "from llama_index.embeddings import SimilarityMode, HuggingFaceEmbedding\n",
    "from liqa.load.format_pdf_reader import FormatPdfReader, ParaTitle\n",
    "from liqa.load.format_node_parser import FormatNodeParser\n",
    "from liqa.load import load_util\n",
    "from liqa.pipline import pipline_utils\n",
    "from liqa import utils\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\"\n",
    "os.environ['TMPDIR'] = os.getcwd()+'/tokenizer'\n",
    "os.environ['TIKTOKEN_CACHE_DIR'] = os.getcwd()+'/tokenizer'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '7'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/nvme/share/share/yangyihe/embedding'  # 它会要求这个目录下面的models目录，还强制找models目录，我在这个embeddings下又建设了一个models目录\n",
    "os.environ[\"LLAMA_INDEX_CACHE_DIR\"] = os.getcwd()+'/tokenizer' # 暂时未知真的有用？ 但偶尔有用\n",
    "\n",
    "# logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "# logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "# 代理\n",
    "os.environ[\"http_proxy\"] = \"http://youhongming.p:you19980819*@10.1.8.50:33128/\"\n",
    "os.environ[\"https_proxy\"] = \"http://youhongming.p:you19980819*@10.1.8.50:33128/\"\n",
    "os.environ[\"HTTP_PROXY\"] = \"http://youhongming.p:you19980819*@10.1.8.50:33128/\"\n",
    "os.environ[\"HTTPS_PROXY\"] = \"http://youhongming.p:you19980819*@10.1.8.50:33128/\"\n",
    "\n",
    "LLM_KEY = \"Baichuan2\"\n",
    "\n",
    "if LLM_KEY == \"Qwen-14B-Chat\":\n",
    "    llm = OpenAI(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        temperature=0.1,\n",
    "        api_key='sk-aJzbu0F3j7bstWlR3e4cA9Db59Ac4f669a9f471aFa66C458',\n",
    "        api_base='https://gf.nekoapi.com/v1'\n",
    "    )\n",
    "elif LLM_KEY == \"Baichuan2\":\n",
    "    from liqa.llms.baichuan2 import Baichuan2LLM\n",
    "    llm = Baichuan2LLM() \n",
    "else:\n",
    "    # tokenizer的下载脚本\n",
    "    import os\n",
    "\n",
    "    os.environ[\"http_proxy\"] = \"http://youhongming.p:you19980819*@10.1.8.50:33128/\"\n",
    "    os.environ[\"https_proxy\"] = \"http://youhongming.p:you19980819*@10.1.8.50:33128/\"\n",
    "    os.environ[\"HTTP_PROXY\"] = \"http://youhongming.p:you19980819*@10.1.8.50:33128/\"\n",
    "    os.environ[\"HTTPS_PROXY\"] = \"http://youhongming.p:you19980819*@10.1.8.50:33128/\"\n",
    "\n",
    "    from transformers import AutoTokenizer\n",
    "    # tok=AutoTokenizer.from_pretrained('THUDM/chatglm3-6b',trust_remote_code=True)\n",
    "    # tok=AutoTokenizer.from_pretrained('Qwen/Qwen-14B-Chat',trust_remote_code=True)\n",
    "    # tok=AutoTokenizer.from_pretrained('internlm/internlm-chat-7b-8k',trust_remote_code=True)\n",
    "    # tok=AutoTokenizer.from_pretrained('internlm/internlm-chat-20b',trust_remote_code=True)\n",
    "    # tok=AutoTokenizer.from_pretrained('OrionStarAI/OrionStar-Yi-34B-Chat',trust_remote_code=True)\n",
    "    \n",
    "\n",
    "    import os\n",
    "    os.environ.pop(\"http_proxy\", None)\n",
    "    os.environ.pop(\"https_proxy\", None)\n",
    "    os.environ.pop(\"HTTP_PROXY\", None)\n",
    "    os.environ.pop(\"HTTPS_PROXY\", None)\n",
    "    from tokenizer.chatglm3_6b.tokenization_chatglm import ChatGLMTokenizer\n",
    "    from tokenizer.internlm_chat_7b_8k.tokenization_internlm import InternLMTokenizer\n",
    "    from tokenizer.Qwen_14B_Chat.tokenization_qwen import QWenTokenizer\n",
    "    from tokenizer.yi_34b_chat.tokenization_yi import YiTokenizer\n",
    "    from llama_index.llms import OpenAILike\n",
    "    \n",
    "    if LLM_KEY == \"Chatglm3-6b\":\n",
    "        tok = ChatGLMTokenizer.from_pretrained('THUDM/chatglm3-6b',local_files_only=True,trust_remote_code=False)\n",
    "        llm = OpenAILike(model='THUDM/chatglm3-6b',\n",
    "                        api_base='http://0.0.0.0:8000/v1',\n",
    "                        tokenizer=tok,\n",
    "                        api_key='api_key',)\n",
    "        print(llm.complete('你好？'))\n",
    "    elif LLM_KEY == \"Internlm_chat_7b_8k\":\n",
    "        tok = InternLMTokenizer.from_pretrained('internlm/internlm-chat-7b-8k',local_files_only=True,trust_remote_code=False)\n",
    "        llm = OpenAILike(model='internlm/internlm-chat-7b-8k',\n",
    "                        api_base='http://0.0.0.0:8001/v1',\n",
    "                        is_chat_model=True,\n",
    "                        tokenizer=tok,\n",
    "                        api_key='api_key',\n",
    "                        max_tokens=1000,\n",
    "                        additional_kwargs={\"stop\":[\"<eoa>\\n\"]} )\n",
    "        print(llm.complete('你好？'))\n",
    "    elif LLM_KEY == \"internlm-chat-20b\":\n",
    "        tok = InternLMTokenizer.from_pretrained('internlm/internlm-chat-20b',local_files_only=True,trust_remote_code=False)\n",
    "        llm = OpenAILike(model='internlm/internlm-chat-20b',\n",
    "                        api_base='http://0.0.0.0:8003/v1',\n",
    "                        is_chat_model=True,\n",
    "                        tokenizer=tok,\n",
    "                        api_key='api_key',\n",
    "                        max_tokens=1000,\n",
    "                        additional_kwargs={\"stop\":[\"<eoa>\\n\"]} )\n",
    "        print(llm.complete('你好？'))        \n",
    "    elif LLM_KEY == \"Qwen-14B-Chat\":\n",
    "        tok=QWenTokenizer.from_pretrained('Qwen/Qwen-14B-Chat',local_files_only=True,trust_remote_code=False)\n",
    "        llm=OpenAILike(model='Qwen/Qwen-14B-Chat',\n",
    "                        api_base='http://0.0.0.0:8002/v1',\n",
    "                        is_chat_model=True,\n",
    "                        tokenizer=tok,\n",
    "                        api_key='api_key',\n",
    "                        max_tokens=1000,\n",
    "                        additional_kwargs={\"stop\":[\"<|im_end|>\",\"<|endoftext|>\"] })\n",
    "        print(llm.complete('你好？'))       \n",
    "    elif LLM_KEY == \"OrionStar-Yi-34B-Chat\":\n",
    "        tok = YiTokenizer.from_pretrained('OrionStarAI/OrionStar-Yi-34B-Chat',local_files_only=True,trust_remote_code=False)\n",
    "        llm = OpenAILike(model='OrionStarAI/OrionStar-Yi-34B-Chat',\n",
    "                        api_base='http://0.0.0.0:8003/v1',\n",
    "                        is_chat_model=True,\n",
    "                        tokenizer=tok,\n",
    "                        api_key='api_key',)\n",
    "        print(llm.complete('你好？'))                   \n",
    "\n",
    "embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"BAAI/bge-large-zh\",\n",
    "    cache_folder=\"/nvme/share/share/yangyihe/embedding\",\n",
    "    embed_batch_size=3\n",
    ")\n",
    "\n",
    "service_context = ServiceContext.from_defaults(llm=llm, embed_model=embed_model)\n",
    "format_vector_index: VectorStoreIndex = pipline_utils.create_format_vector_index(\n",
    "    input_dir=\"liqa/dataset/right\", service_context=service_context, use_storage = True\n",
    ")\n",
    "\n",
    "# simple_vector_index = pipline_utils.create_simple_vector_index(\n",
    "#     input_dir=\"liqa/dataset/right\", service_context=service_context, use_storage = False\n",
    "# )\n",
    "\n",
    "# leaf_vector_index = pipline_utils.create_leaf_vector_index(\n",
    "#     input_dir=\"liqa/dataset/right\", service_context=service_context, use_storage = False\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from liqa.pipline import pipline_utils\n",
    "from liqa.evaluation.eval_utils import EvalTool\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "def get_prompt(index:int):\n",
    "    if index == 0:\n",
    "        from liqa.query.chinese_prompt import DEFAULT_TEXT_QA_PROMPT, DEFAULT_REFINE_PROMPT\n",
    "    if index == 1:\n",
    "        from liqa.query.chinese_prompt1 import DEFAULT_TEXT_QA_PROMPT, DEFAULT_REFINE_PROMPT\n",
    "    if index == 2:\n",
    "        from liqa.query.chinese_prompt2 import DEFAULT_TEXT_QA_PROMPT, DEFAULT_REFINE_PROMPT\n",
    "    if index == 3:\n",
    "        from liqa.query.chinese_prompt3 import DEFAULT_TEXT_QA_PROMPT, DEFAULT_REFINE_PROMPT\n",
    "    if index == 4:\n",
    "        from liqa.query.chinese_prompt4 import DEFAULT_TEXT_QA_PROMPT, DEFAULT_REFINE_PROMPT\n",
    "    if index == 5:\n",
    "        from liqa.query.chinese_prompt5 import DEFAULT_TEXT_QA_PROMPT, DEFAULT_REFINE_PROMPT\n",
    "    if index == 6:\n",
    "        from liqa.query.chinese_prompt6 import DEFAULT_TEXT_QA_PROMPT, DEFAULT_REFINE_PROMPT\n",
    "    if index == 7:\n",
    "        from liqa.query.chinese_prompt7 import DEFAULT_TEXT_QA_PROMPT, DEFAULT_REFINE_PROMPT\n",
    "    if index == 8:\n",
    "        from liqa.query.chinese_prompt8 import DEFAULT_TEXT_QA_PROMPT, DEFAULT_REFINE_PROMPT\n",
    "    if index == 9:\n",
    "        from liqa.query.chinese_prompt9 import DEFAULT_TEXT_QA_PROMPT, DEFAULT_REFINE_PROMPT                                \n",
    "\n",
    "    return DEFAULT_TEXT_QA_PROMPT, DEFAULT_REFINE_PROMPT\n",
    "\n",
    "dataframe = EvalTool.read_source_table()\n",
    "dataframe = dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name='test_large.xlsx'\n",
    "from liqa.evaluation.retriever_file import eval_retriever_file\n",
    "from liqa.evaluation.retriever_file import FileRetrievalStats\n",
    "ans=eval_retriever_file(retriever=EvalTool.get_reranker(format_vector_index) ,top_k=2,file_name=file_name)\n",
    "error_rate, confuse_pairs = FileRetrievalStats.analyze(df=ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "construct_response_table query_engine id:139775390081824, curt_index:30\n",
      "construct_response_table query_engine id:139775390081824, curt_index:60\n",
      "construct_response_table query_engine id:139775390081824, curt_index:90\n",
      "construct_response_table query_engine id:139775390081824, curt_index:120\n",
      "construct_response_table query_engine id:139775390081824, curt_index:150\n",
      "Baichuan2 prompt_5 Similarity mean: 0.9287109090621503, Similarity>0.9: 0.79\n",
      "construct_response_table query_engine id:139771294832464, curt_index:30\n",
      "construct_response_table query_engine id:139771294832464, curt_index:60\n",
      "construct_response_table query_engine id:139771294832464, curt_index:90\n",
      "construct_response_table query_engine id:139771294832464, curt_index:120\n",
      "construct_response_table query_engine id:139771294832464, curt_index:150\n",
      "Baichuan2 prompt_7 Similarity mean: 0.9359953231905184, Similarity>0.9: 0.85\n"
     ]
    }
   ],
   "source": [
    "# 不带retriver的\n",
    "import asyncio\n",
    "from llama_index.response_synthesizers import (\n",
    "    ResponseMode,\n",
    ")\n",
    "\n",
    "async def test_data_by_prompt(llm_name, index:int, dataframe):\n",
    "    text_qa_template, refine_template = get_prompt(index)\n",
    "    \n",
    "    dataframe = await EvalTool.construct_response_table(format_vector_index.as_query_engine(\n",
    "            response_mode = ResponseMode.SIMPLE_SUMMARIZE,\n",
    "            text_qa_template = text_qa_template,\n",
    "            refine_template = refine_template,\n",
    "            service_context = service_context\n",
    "        ), dataframe)\n",
    "\n",
    "    dataframe[EvalTool.Key_Similarity] = dataframe.apply(\n",
    "        lambda x : EvalTool.similarity_score(x[EvalTool.Key_References], x[EvalTool.Key_Responses]), axis=1\n",
    "    )\n",
    "\n",
    "    dataframe.to_csv(f\"temp_data/{llm_name}_dataframe_prompt_{index}.csv\")\n",
    "    s_percent = round((dataframe[EvalTool.Key_Similarity] > 0.9).sum() / len(dataframe), 2)\n",
    "    print(f\"{llm_name} prompt_{index} Similarity mean: {dataframe[EvalTool.Key_Similarity].mean()}, Similarity>0.9: {s_percent}\")\n",
    "\n",
    "for i in [5, 7]:\n",
    "# for i in [7]:\n",
    "    asyncio.run(test_data_by_prompt(LLM_KEY, i, dataframe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### baichuan2_13b\n",
    "baichuan2_13b prompt_0 Similarity mean: 0.9206298944858948, Similarity>0.9: 0.68  \n",
    "baichuan2_13b prompt_1 Similarity mean: 0.9243395878242535, Similarity>0.9: 0.72  \n",
    "baichuan2_13b prompt_2 Similarity mean: 0.9231398381468222, Similarity>0.9: 0.75  \n",
    "baichuan2_13b prompt_3 Similarity mean: 0.9185603010534343, Similarity>0.9: 0.67  \n",
    "baichuan2_13b prompt_4 Similarity mean: 0.9330465202260947, Similarity>0.9: 0.83  \n",
    "baichuan2_13b prompt_5 Similarity mean: 0.9310741605041983, Similarity>0.9: 0.85  \n",
    "baichuan2_13b prompt_6 Similarity mean: 0.9230675937225877, Similarity>0.9: 0.72  \n",
    "baichuan2_13b prompt_7 Similarity mean: 0.9230675937225877, Similarity>0.9: 0.87  \n",
    "\n",
    "### Qwen-14B-Chat  \n",
    "Qwen-14B-Chat prompt_0 Similarity mean: 0.9227873312154024, Similarity>0.9: 0.68  \n",
    "Qwen-14B-Chat prompt_1 Similarity mean: 0.9294164407658203, Similarity>0.9: 0.71  \n",
    "Qwen-14B-Chat prompt_2 Similarity mean: 0.9285036919532919, Similarity>0.9: 0.75  \n",
    "Qwen-14B-Chat prompt_3 Similarity mean: 0.9229634015648194, Similarity>0.9: 0.7  \n",
    "Qwen-14B-Chat prompt_4 Similarity mean: 0.9296638696038633, Similarity>0.9: 0.75  \n",
    "Qwen-14B-Chat prompt_5 Similarity mean: 0.936327977053431, Similarity>0.9: 0.86  \n",
    "Qwen-14B-Chat prompt_6 Similarity mean: 0.9268301058781749, Similarity>0.9: 0.78  \n",
    "Qwen-14B-Chat prompt_7 Similarity mean: 0.936312818896312, Similarity>0.9: 0.85\n",
    "\n",
    "### chatglm3-6b  \n",
    "chatglm3-6b prompt_0 Similarity mean: 0.9260715827534315, Similarity>0.9: 0.72  \n",
    "chatglm3-6b prompt_1 Similarity mean: 0.928699525191309, Similarity>0.9: 0.74  \n",
    "chatglm3-6b prompt_3 Similarity mean: 0.9256637582478308, Similarity>0.9: 0.74  \n",
    "chatglm3-6b prompt_4 Similarity mean: 0.9234209123229621, Similarity>0.9: 0.73  \n",
    "chatglm3-6b prompt_5 Similarity mean: 0.924024504540342, Similarity>0.9: 0.73  \n",
    "chatglm3-6b prompt_6 Similarity mean: 0.929421537480509, Similarity>0.9: 0.79  \n",
    "chatglm3-6b prompt_7 Similarity mean: 0.9280174611599588, Similarity>0.9: 0.76  \n",
    "\n",
    "### Internlm_chat_7b_8k  \n",
    "Internlm_chat_7b_8k prompt_0 Similarity mean: 0.8980771916433794, Similarity>0.9: 0.56  \n",
    "Internlm_chat_7b_8k prompt_1 Similarity mean: 0.92645604504688, Similarity>0.9: 0.76  \n",
    "Internlm_chat_7b_8k prompt_2 Similarity mean: 0.940475792113533, Similarity>0.9: 0.81  \n",
    "Internlm_chat_7b_8k prompt_3 Similarity mean: 0.9296439634279264, Similarity>0.9: 0.75  \n",
    "Internlm_chat_7b_8k prompt_4 Similarity mean: 0.9333530476184667, Similarity>0.9: 0.8  \n",
    "Internlm_chat_7b_8k prompt_5 Similarity mean: 0.9436251493986421, Similarity>0.9: 0.82  \n",
    "Internlm_chat_7b_8k prompt_6 Similarity mean: 0.9362305075331976, Similarity>0.9: 0.81  \n",
    "Internlm_chat_7b_8k prompt_7 Similarity mean: 0.9418512650383399, Similarity>0.9: 0.81  \n",
    "\n",
    "### OrionStar-Yi-34B-Chat  \n",
    "OrionStar-Yi-34B-Chat prompt_0 Similarity mean: 0.9270971083316923, Similarity>0.9: 0.75  \n",
    "OrionStar-Yi-34B-Chat prompt_1 Similarity mean: 0.9284401800869978, Similarity>0.9: 0.75  \n",
    "OrionStar-Yi-34B-Chat prompt_2 Similarity mean: 0.9292302254341365, Similarity>0.9: 0.79  \n",
    "OrionStar-Yi-34B-Chat prompt_3 Similarity mean: 0.9262625478329085, Similarity>0.9: 0.77  \n",
    "OrionStar-Yi-34B-Chat prompt_4 Similarity mean: 0.9177818627418569, Similarity>0.9: 0.7  \n",
    "OrionStar-Yi-34B-Chat prompt_5 Similarity mean: 0.918178130331102, Similarity>0.9: 0.7  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List, Optional, Sequence\n",
    "\n",
    "from llama_index.prompts.prompt_utils import get_biggest_prompt\n",
    "from llama_index.response_synthesizers.refine import Refine\n",
    "from llama_index.types import RESPONSE_TEXT_TYPE\n",
    "from llama_index.prompts import BasePromptTemplate\n",
    "\n",
    "from typing import Any, List, Optional, Sequence\n",
    "from llama_index.prompts import BasePromptTemplate\n",
    "from llama_index.response_synthesizers import BaseSynthesizer\n",
    "from llama_index.service_context import ServiceContext\n",
    "from llama_index.core import BaseQueryEngine, BaseRetriever\n",
    "\n",
    "from llama_index.prompts.default_prompt_selectors import (\n",
    "    DEFAULT_REFINE_PROMPT_SEL,\n",
    "    DEFAULT_TEXT_QA_PROMPT_SEL,\n",
    "    DEFAULT_TREE_SUMMARIZE_PROMPT_SEL,\n",
    ")\n",
    "from llama_index.prompts.default_prompts import DEFAULT_SIMPLE_INPUT_PROMPT\n",
    "\n",
    "\n",
    "class MyCompactAndRefine(Refine):\n",
    "    \"\"\"Refine responses across compact text chunks.\"\"\"\n",
    "\n",
    "    async def aget_response(\n",
    "        self,\n",
    "        query_str: str,\n",
    "        text_chunks: Sequence[str],\n",
    "        prev_response: Optional[RESPONSE_TEXT_TYPE] = None,\n",
    "        **response_kwargs: Any,\n",
    "    ) -> RESPONSE_TEXT_TYPE:\n",
    "        compact_texts = self._make_compact_text_chunks(query_str, text_chunks)\n",
    "        print(1111)\n",
    "        return await super().aget_response(\n",
    "            query_str=query_str,\n",
    "            text_chunks=compact_texts,\n",
    "            prev_response=prev_response,\n",
    "            **response_kwargs,\n",
    "        )\n",
    "\n",
    "    def get_response(\n",
    "        self,\n",
    "        query_str: str,\n",
    "        text_chunks: Sequence[str],\n",
    "        prev_response: Optional[RESPONSE_TEXT_TYPE] = None,\n",
    "        **response_kwargs: Any,\n",
    "    ) -> RESPONSE_TEXT_TYPE:\n",
    "        \"\"\"Get compact response.\"\"\"\n",
    "        # use prompt helper to fix compact text_chunks under the prompt limitation\n",
    "        # TODO: This is a temporary fix - reason it's temporary is that\n",
    "        # the refine template does not account for size of previous answer.\n",
    "        new_texts = self._make_compact_text_chunks(query_str, text_chunks)\n",
    "        print(2222)\n",
    "        return super().get_response(\n",
    "            query_str=query_str,\n",
    "            text_chunks=new_texts,\n",
    "            prev_response=prev_response,\n",
    "            **response_kwargs,\n",
    "        )\n",
    "\n",
    "    def _make_compact_text_chunks(\n",
    "        self, query_str: str, text_chunks: Sequence[str]\n",
    "    ) -> List[str]:\n",
    "        text_qa_template = self._text_qa_template.partial_format(query_str=query_str)\n",
    "        refine_template = self._refine_template.partial_format(query_str=query_str)\n",
    "\n",
    "        max_prompt = get_biggest_prompt([text_qa_template, refine_template])\n",
    "        return self._service_context.prompt_helper.repack(max_prompt, text_chunks)\n",
    "\n",
    "\n",
    "def create_synthesizer(\n",
    "    service_context: Optional[ServiceContext] = None,\n",
    "    text_qa_template: Optional[BasePromptTemplate] = None,\n",
    "    refine_template: Optional[BasePromptTemplate] = None,\n",
    "    summary_template: Optional[BasePromptTemplate] = None,\n",
    "    simple_template: Optional[BasePromptTemplate] = None,\n",
    ") -> BaseSynthesizer:\n",
    "    \"\"\"Get a response synthesizer.\"\"\"\n",
    "    text_qa_template = text_qa_template or DEFAULT_TEXT_QA_PROMPT_SEL\n",
    "    refine_template = refine_template or DEFAULT_REFINE_PROMPT_SEL\n",
    "    simple_template = simple_template or DEFAULT_SIMPLE_INPUT_PROMPT\n",
    "    summary_template = summary_template or DEFAULT_TREE_SUMMARIZE_PROMPT_SEL\n",
    "\n",
    "    return MyCompactAndRefine(\n",
    "        service_context=service_context,\n",
    "        text_qa_template=text_qa_template,\n",
    "        refine_template=refine_template,\n",
    "    )\n",
    "\n",
    "\n",
    "def create_query_engine(\n",
    "    vector_index: VectorStoreIndex,\n",
    "    retriever: BaseRetriever = None,\n",
    "    service_context: Optional[ServiceContext] = None,\n",
    "    **kwargs: Any,\n",
    ") -> BaseQueryEngine:\n",
    "    # NOTE: lazy import\n",
    "    from llama_index.query_engine.retriever_query_engine import RetrieverQueryEngine\n",
    "    \n",
    "    kwargs[\"service_context\"] = service_context or vector_index._service_context\n",
    "    kwargs[\"retriever\"] = retriever or vector_index.as_retriever(**kwargs)\n",
    "    \n",
    "    return RetrieverQueryEngine.from_args(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_qa_template, refine_template = get_prompt(5)\n",
    "\n",
    "retriever = format_vector_index.as_retriever()\n",
    "\n",
    "nodes = retriever.retrieve(\"今天天气怎么样\")\n",
    "\n",
    "nodes[0].node.excluded_llm_metadata_keys\n",
    "\n",
    "\n",
    "query_engine = format_vector_index.as_query_engine(\n",
    "    text_qa_template = text_qa_template,\n",
    "    refine_template = refine_template,\n",
    "    service_context = service_context\n",
    ")\n",
    "\n",
    "query_engine.query(\"今天天气怎么样\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv(\"/home/sunshangbin.p/workspace/llamaindexqa/temp_data/baichuan_dataframe_prompt_7.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# dataframe = dataframe[[\"Questions\", \"References\", \"Responses\", \"Similarity\"]]\n",
    "dataframe[dataframe[\"Similarity\"] < 0.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from liqa.evaluation.eval_utils import EvalTool\n",
    "dataframe[EvalTool.Key_Similarity] = dataframe.apply(\n",
    "    lambda x : EvalTool.similarity_score(x[EvalTool.Key_References], x[EvalTool.Key_Responses]), axis=1\n",
    ")\n",
    "\n",
    "s_percent = round((dataframe[EvalTool.Key_Similarity] > 0.9).sum() / len(dataframe), 2)\n",
    "print(f\"baichuan prompt Similarity mean: {dataframe[EvalTool.Key_Similarity].mean()}, Similarity>0.9: {s_percent}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "async def test(format_vector_index, dataframe):\n",
    "    async for line in EvalTool.query_engine_aquery_batch(format_vector_index, dataframe):\n",
    "        print(line)\n",
    "\n",
    "asyncio.run(test(format_vector_index.as_query_engine(), dataframe[EvalTool.Key_Questions].to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from llama_index.response_synthesizers import ResponseMode\n",
    "\n",
    "async def test_data_by_response_mode(response_mode:ResponseMode, dataframe):\n",
    "    text_qa_template, refine_template = get_prompt(2)\n",
    "    print(1111)\n",
    "    dataframe = await EvalTool.construct_response_table(format_vector_index.as_query_engine(\n",
    "            response_mode = response_mode,\n",
    "            text_qa_template = text_qa_template,\n",
    "            refine_template = refine_template\n",
    "        ), dataframe)\n",
    "\n",
    "    dataframe[EvalTool.Key_Similarity] = dataframe.apply(\n",
    "        lambda x : EvalTool.similarity_score(x[EvalTool.Key_References], x[EvalTool.Key_Responses]), axis=1\n",
    "    )\n",
    "\n",
    "    dataframe.to_csv(f\"data/dataframe_path_prompt_{response_mode.name}.csv\")\n",
    "    s_percent = round((dataframe[EvalTool.Key_Similarity] > 0.9).sum() / len(dataframe), 2)\n",
    "    print(f\"gpt3.5 prompt_{response_mode.name} Similarity mean: {dataframe[EvalTool.Key_Similarity].mean()}, Similarity>0.9: {s_percent}\")\n",
    "\n",
    "    \n",
    "async def helloworld(dataframe):\n",
    "    print('enter helloworld')\n",
    "    task = [test_data_by_prompt(response_mode, dataframe) for response_mode in [ResponseMode.COMPACT_ACCUMULATE, ResponseMode.SIMPLE_SUMMARIZE]]\n",
    "    ret = await asyncio.gather(*task)\n",
    "    print('exit helloworld')\n",
    "    return ret\n",
    "\n",
    "asyncio.run(helloworld(dataframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.response_synthesizers import ResponseMode\n",
    "\n",
    "for response_mode in [ResponseMode.COMPACT_ACCUMULATE, ResponseMode.SIMPLE_SUMMARIZE]:\n",
    "    text_qa_template, refine_template = get_prompt(1)\n",
    "    dataframe = EvalTool.construct_response_table(format_vector_index.as_query_engine(\n",
    "            response_mode = response_mode,\n",
    "            text_qa_template = text_qa_template,\n",
    "            refine_template = refine_template\n",
    "        ), dataframe)\n",
    "    \n",
    "    dataframe[EvalTool.Key_Similarity] = dataframe.apply(\n",
    "        lambda x : EvalTool.similarity_score(x[EvalTool.Key_References], x[EvalTool.Key_Responses]), axis=1\n",
    "    )\n",
    "    \n",
    "    dataframe.to_csv(f\"data/dataframe_path_top{response_mode}.csv\")\n",
    "    s_percent = round((dataframe[EvalTool.Key_Similarity] > 0.9).sum() / len(dataframe), 2)\n",
    "    print(f\"gpt3.5 Similarity mean: {dataframe[EvalTool.Key_Similarity].mean()}, Similarity>0.9: {s_percent}\")\n",
    "    \n",
    "f\"{ResponseMode.COMPACT_ACCUMULATE.name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for top_k in [1, 2, 3]:\n",
    "    text_qa_template, refine_template = get_prompt(1)\n",
    "    dataframe = EvalTool.construct_response_table(format_vector_index.as_query_engine(\n",
    "            retriever = format_vector_index.as_retriever(similarity_top_k=top_k),\n",
    "            text_qa_template = text_qa_template,\n",
    "            refine_template = refine_template\n",
    "        ), dataframe)\n",
    "    \n",
    "    dataframe[EvalTool.Key_Similarity] = dataframe.apply(\n",
    "        lambda x : EvalTool.similarity_score(x[EvalTool.Key_References], x[EvalTool.Key_Responses]), axis=1\n",
    "    )\n",
    "    \n",
    "    dataframe.to_csv(f\"data/dataframe_path_top{top_k}.csv\")\n",
    "    s_percent = round((dataframe[EvalTool.Key_Similarity] > 0.9).sum() / len(dataframe), 2)\n",
    "    print(f\"gpt3.5 Similarity mean: {dataframe[EvalTool.Key_Similarity].mean()}, Similarity>0.9: {s_percent}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv(\"/home/sunshangbin.p/workspace/llamaindexqa/liqa/evaluation/data/dataframe_path_top1.csv\")\n",
    "s_percent = round((dataframe[EvalTool.Key_Similarity] > 0.9).sum() / len(dataframe), 2)\n",
    "print(f\"gpt3.5 Similarity mean: {dataframe[EvalTool.Key_Similarity].mean()}, Similarity>0.9: {s_percent}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe[EvalTool.Key_Similarity] = dataframe.apply(\n",
    "    lambda x : EvalTool.similarity_score(x[EvalTool.Key_References], x[EvalTool.Key_Responses]), axis=1\n",
    ")\n",
    "\n",
    "# dataframe.to_csv(f\"data/dataframe_path_top{top_k}.csv\")\n",
    "s_percent = round((dataframe[EvalTool.Key_Similarity] > 0.9).sum() / len(dataframe), 2)\n",
    "print(f\"gpt3.5 Similarity mean: {dataframe[EvalTool.Key_Similarity].mean()}, Similarity>0.9: {s_percent}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text chunk功能  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from liqa.load.format_pdf_reader import FormatPdfReader, ParaTitle\n",
    "from liqa.pipline import pipline_utils\n",
    "format_vector_index = pipline_utils.create_format_vector_index(\n",
    "    input_dir=\"liqa/dataset/right\", service_context=service_context, use_storage = False\n",
    ")\n",
    "\n",
    "simple_vector_index = pipline_utils.create_simple_vector_index(\n",
    "    input_dir=\"liqa/dataset/right\", service_context=service_context, use_storage = False\n",
    ")\n",
    "\n",
    "leaf_vector_index = pipline_utils.create_leaf_vector_index(\n",
    "    input_dir=\"liqa/dataset/right\", service_context=service_context, use_storage = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from liqa.load.format_pdf_reader import FormatPdfReader, ParaTitle\n",
    "from liqa.load.format_node_parser import FormatNodeParser\n",
    "from liqa.load import load_util\n",
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.node_parser import get_leaf_nodes, get_root_nodes\n",
    "from liqa import utils\n",
    "documents = SimpleDirectoryReader(input_dir=utils.convert_path_to_abspath('liqa/dataset/right/source'), file_extractor={\".pdf\": FormatPdfReader()}).load_data()\n",
    "parser = FormatNodeParser.from_defaults(parent_has_child_content=False)\n",
    "nodes= parser.get_nodes_from_documents(documents)\n",
    "format_nodes= [node for node in get_leaf_nodes(nodes)]\n",
    "format_index = VectorStoreIndex(format_nodes,service_context=service_context,show_progess=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import QueryBundle\n",
    "\n",
    "# import NodeWithScore\n",
    "from llama_index.schema import NodeWithScore\n",
    "\n",
    "# Retrievers\n",
    "from llama_index.retrievers import (\n",
    "    BaseRetriever,\n",
    "    VectorIndexRetriever,\n",
    "    KGTableRetriever,\n",
    ")\n",
    "\n",
    "from typing import List\n",
    "\n",
    "class YyhRetriever(BaseRetriever):\n",
    "    def __init__(self, index_retriever, base_nodes):#bm25_retriever,\n",
    "        self.index_retriever = index_retriever\n",
    "        # self.bm25_retriever = None#bm25_retriever\n",
    "        self.base_nodes=base_nodes\n",
    "\n",
    "    def _retrieve(self, query, **kwargs):\n",
    "        index_nodes = self.index_retriever.retrieve(query, **kwargs)\n",
    "        # bm25_nodes = self.bm25_retriever.retrieve(query, **kwargs)\n",
    "        \n",
    "        selected_ids=[index_node.node.index_id  for index_node in index_nodes]\n",
    "        \n",
    "        seen_ids = set()\n",
    "        vector_nodes = []\n",
    "        for id in selected_ids:\n",
    "            if id not in seen_ids:\n",
    "                seen_ids.add(id)\n",
    "                vector_nodes.extend(node for node in self.base_nodes if node.node_id == id)\n",
    "        \n",
    "        result = vector_nodes\n",
    "        \n",
    "        \n",
    "        return [NodeWithScore(node=node,score=1)for node in result]\n",
    "        # combine the two lists of nodes\n",
    "        # all_nodes = []\n",
    "        # node_ids = set()\n",
    "        # for n in bm25_nodes + vector_nodes:\n",
    "        #     if n.node.node_id not in node_ids:\n",
    "        #         all_nodes.append(n)\n",
    "        #         node_ids.add(n.node.node_id)\n",
    "        # return all_nodes\n",
    "\n",
    "\n",
    "class YyhBm25Retriever(BaseRetriever):\n",
    "    def __init__(self, index_retriever,base_nodes,bm25_retriever):#,\n",
    "        self.index_retriever = index_retriever\n",
    "        self.bm25_retriever = bm25_retriever\n",
    "        self.base_nodes=base_nodes\n",
    "\n",
    "    def _retrieve(self, query, **kwargs):\n",
    "        index_nodes = self.index_retriever.retrieve(query, **kwargs)\n",
    "        bm25_nodes = self.bm25_retriever.retrieve(query, **kwargs)\n",
    "        \n",
    "        selected_ids=[index_node.node.index_id  for index_node in index_nodes]\n",
    "        \n",
    "        seen_ids = set()\n",
    "        vector_nodes = []\n",
    "        for id in selected_ids:\n",
    "            if id not in seen_ids:\n",
    "                seen_ids.add(id)\n",
    "                vector_nodes.extend(node for node in self.base_nodes if node.node_id == id)\n",
    "        \n",
    "\n",
    "class yyhbm25Retriever(BaseRetriever):\n",
    "    def __init__(self, vector_retriever, bm25_retriever,reranker):\n",
    "        self.vector_retriever = vector_retriever\n",
    "        self.reranker=reranker\n",
    "        self.bm25_retriever = bm25_retriever\n",
    "\n",
    "    def _retrieve(self, query, **kwargs):\n",
    "        bm25_nodes = self.bm25_retriever.retrieve(query, **kwargs)\n",
    "        vector_nodes = self.vector_retriever.retrieve(query, **kwargs)\n",
    "        \n",
    "        \n",
    "        all_nodes = []\n",
    "        node_ids = set()\n",
    "        for n in bm25_nodes + vector_nodes:\n",
    "            if n.node.node_id not in node_ids:\n",
    "                all_nodes.append(n)\n",
    "                node_ids.add(n.node.node_id)\n",
    "        all_nodes=self.reranker.postprocess_nodes(all_nodes,query_bundle=query)\n",
    "        return all_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from liqa.evaluation.eval_utils import EvalTool\n",
    "\n",
    "dataframe = EvalTool.read_source_table()\n",
    "def test_infile(vector_index, dataframe):\n",
    "    dataframe = EvalTool.construct_retriever_table(vector_index.as_retriever(similarity_top_k=1), dataframe)\n",
    "    in_file_percent = round(dataframe[EvalTool.Key_InFile].sum() / len(dataframe), 2)\n",
    "    print(in_file_percent)\n",
    "    return dataframe\n",
    "    \n",
    "format_df = test_infile(format_vector_index, dataframe)\n",
    "simple_df = test_infile(simple_vector_index, dataframe)\n",
    "leaf_df = test_infile(leaf_vector_index, dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from liqa.evaluation.eval_utils import EvalTool\n",
    "# from liqa.query.chinese_prompt import DEFAULT_TEXT_QA_PROMPT, DEFAULT_REFINE_PROMPT\n",
    "from liqa.query.chinese_prompt1 import DEFAULT_TEXT_QA_PROMPT, DEFAULT_REFINE_PROMPT\n",
    "\n",
    "dataframe_path = \"openai_dataframe.csv\"\n",
    "if os.path.exists(dataframe_path) and False:\n",
    "    dataframe = pd.read_csv(dataframe_path)\n",
    "else:\n",
    "    dataframe = EvalTool.read_source_table()\n",
    "    # dataframe = EvalTool.construct_retriever_table(format_vector_index.as_retriever(similarity_top_k=1), dataframe)\n",
    "    # in_file_percent = round(dataframe[EvalTool.Key_InFile].sum() / len(dataframe), 2)\n",
    "    dataframe = EvalTool.construct_response_table(format_vector_index.as_query_engine(\n",
    "            text_qa_template = DEFAULT_TEXT_QA_PROMPT,\n",
    "            refine_template = DEFAULT_REFINE_PROMPT\n",
    "        ), dataframe)\n",
    "    dataframe.to_csv(dataframe_path)\n",
    "    \n",
    "\n",
    "in_file_percent\n",
    "# dataframe[EvalTool.Key_Similarity] = dataframe.apply(\n",
    "#     lambda x : EvalTool.similarity_score(x[EvalTool.Key_References], x[EvalTool.Key_Responses]), axis=1\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.indices.vector_store.retrievers import VectorIndexRetriever\n",
    "retriever:VectorIndexRetriever = format_vector_index.as_retriever(similarity_top_k=3)\n",
    "\n",
    "question = \"证监会对ipo申请通过注册，是不是代表对投资者收益有了保证？\"\n",
    "nodes = retriever.retrieve(question)\n",
    "for node in nodes:\n",
    "    # print(EvalTool.similarity_score(question, node.get_content()))\n",
    "    print(node.get_score(), node.get_content())\n",
    "    \n",
    "dataframe[dataframe[EvalTool.Key_InFile] == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe[EvalTool.Key_Similarity] = dataframe.apply(\n",
    "    lambda x : EvalTool.similarity_score(x[EvalTool.Key_References], x[EvalTool.Key_Responses]), axis=1\n",
    ")\n",
    "\n",
    "s_percent = round((dataframe[EvalTool.Key_Similarity] > 0.9).sum() / len(dataframe), 2)\n",
    "in_file_percent = round(dataframe[EvalTool.Key_InFile].sum() / len(dataframe), 2)\n",
    "print(f\"gpt3.5 In file:{in_file_percent} Similarity mean: {dataframe[EvalTool.Key_Similarity].mean()}, Similarity>0.9: {s_percent}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatchat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
